{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QABot dialog analysis\n",
    "\n",
    "Why do people drop off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "DATEFORMAT = \"%m/%d/%Y, %I:%M:%S %p\"\n",
    "CONDITIONS = ['llm-chatbot', 'llm-qa-bot', 'reading', 'teacher-qa-bot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "file = \"pilot_09_06.json\"\n",
    "with open(\"./prolific_logs/\" + file) as f:\n",
    "    logs = json.load(f)\n",
    "logs = [logs[\"logs\"][l] for l in logs[\"logs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_dialog(log):\n",
    "    messages = []\n",
    "    for m in log[\"chatLog\"][\"current\"][\"main\"]:\n",
    "        time = datetime.strptime(m[\"date\"], DATEFORMAT)\n",
    "        messages.append((time, m[\"text\"]))\n",
    "    messages.sort()\n",
    "    messages = [m[1] for m in messages]\n",
    "    return messages\n",
    "\n",
    "# helper function\n",
    "def get_avg_std_err(vals):\n",
    "    mean = np.mean(vals)\n",
    "    std_dev = np.std(vals, ddof=1)  # Using ddof=1 for sample standard deviation\n",
    "    \n",
    "    # Calculate the standard error using the formula: standard deviation / sqrt(sample size)\n",
    "    standard_error = std_dev / np.sqrt(len(vals))\n",
    "    return mean, standard_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering\n",
      "all logs: 101\n",
      "llm-chatbot 26\n",
      "llm-qa-bot 25\n",
      "reading 25\n",
      "teacher-qa-bot 25\n"
     ]
    }
   ],
   "source": [
    "# only consider completed sessions\n",
    "print(\"Before filtering\")\n",
    "print(\"all logs:\", len(logs))\n",
    "for c in CONDITIONS:\n",
    "    print(c, len([l for l in logs if l[\"condition\"] == c]))\n",
    "\n",
    "completed = [l for l in logs if l[\"completedSurvey\"] == True]\n",
    "drop = [l for l in logs if l[\"completedSurvey\"] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When did QA condition drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_qa_llm = [l for l in drop if l[\"condition\"] == c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completedSurvey\n",
      "condition\n",
      "consent\n",
      "initializationTimes\n",
      "isMobile\n",
      "lessonModalTimes\n",
      "localCreationTime\n",
      "modalTimes\n",
      "optOut\n",
      "scrollLogs\n",
      "state\n",
      "surveyToken\n",
      "tabSwitches\n",
      "timestamps\n",
      "userAgent\n",
      "uuid\n"
     ]
    }
   ],
   "source": [
    "for k in drop_qa_llm[0]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'creation': '9/6/2023, 4:18:55 PM', 'enterMain': '9/6/2023, 4:19:13 PM', 'enterWelcome': '9/6/2023, 4:19:04 PM'}\n",
      "{'creation': '9/6/2023, 6:11:26 PM', 'enterKnowledge': '9/6/2023, 6:16:31 PM', 'enterMain': '9/6/2023, 6:11:34 PM', 'enterWelcome': '9/6/2023, 6:11:31 PM'}\n",
      "{'creation': '9/6/2023, 6:57:12 PM', 'enterMain': '9/6/2023, 6:57:30 PM', 'enterWelcome': '9/6/2023, 6:57:28 PM'}\n",
      "{'creation': '9/6/2023, 2:06:58 PM', 'enterMain': '9/6/2023, 2:07:19 PM', 'enterWelcome': '9/6/2023, 2:07:10 PM'}\n",
      "{'creation': '9/6/2023, 6:11:31 PM', 'enterMain': '9/6/2023, 6:13:55 PM', 'enterWelcome': '9/6/2023, 6:13:03 PM'}\n",
      "{'creation': '9/6/2023, 2:53:21 PM', 'enterMain': '9/6/2023, 2:54:31 PM', 'enterWelcome': '9/6/2023, 2:54:08 PM'}\n",
      "{'creation': '9/6/2023, 2:16:43\\u202fPM'}\n",
      "{'creation': '9/6/2023, 6:11:36 PM', 'enterMain': '9/6/2023, 6:12:55 PM', 'enterWelcome': '9/6/2023, 6:12:27 PM'}\n",
      "{'creation': '9/6/2023, 2:27:38 PM', 'enterMain': '9/6/2023, 2:28:52 PM', 'enterWelcome': '9/6/2023, 2:28:14 PM'}\n",
      "{'creation': '9/6/2023, 2:54:11 PM', 'enterMain': '9/6/2023, 3:20:03 PM', 'enterWelcome': '9/6/2023, 3:18:50 PM'}\n",
      "{'creation': '9/6/2023, 2:56:32 PM', 'enterMain': '9/6/2023, 2:56:51 PM', 'enterWelcome': '9/6/2023, 2:56:40 PM'}\n",
      "{'creation': '9/6/2023, 2:52:31 PM', 'enterMain': '9/6/2023, 2:54:12 PM', 'enterWelcome': '9/6/2023, 2:54:10 PM'}\n",
      "{'creation': '9/6/2023, 2:53:04 PM', 'enterMain': '9/6/2023, 2:53:47 PM', 'enterWelcome': '9/6/2023, 2:53:18 PM'}\n",
      "{'creation': '9/6/2023, 3:58:03 PM', 'enterMain': '9/6/2023, 3:58:40 PM', 'enterWelcome': '9/6/2023, 3:58:21 PM'}\n",
      "{'creation': '9/6/2023, 3:45:25 PM', 'enterMain': '9/6/2023, 3:45:52 PM', 'enterWelcome': '9/6/2023, 3:45:38 PM'}\n",
      "{'creation': '9/6/2023, 2:24:41 PM', 'enterMain': '9/6/2023, 2:25:15 PM', 'enterWelcome': '9/6/2023, 2:25:08 PM'}\n"
     ]
    }
   ],
   "source": [
    "for k in drop_qa_llm:\n",
    "    if \"enterMain\" in k[\"timestamps\"]:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAfter filtering\")\n",
    "print(\"completed logs:\", len(logs))\n",
    "for c in CONDITIONS:\n",
    "    print(c, len([l for l in logs if l[\"condition\"] == c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many actions for drop-off students\n",
    "for c in CONDITIONS:\n",
    "    group = [l for l in logs if l[\"condition\"] == c]\n",
    "    for log in group:\n",
    "        print(len(compile_dialog(log)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check exam tab switches on test pages\n",
    "# check other statistics that Meng computed befre\n",
    "# NUMBER OF INTERACTIONS\n",
    "# TOTAL WORDS WRITTEN\n",
    "# TIME ON LESSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num_messages(log):\n",
    "    if log[\"condition\"] == \"treatment\":\n",
    "        return sum([e[\"isUser\"] for e in log[\"chatLog\"][\"current\"][\"main\"]])\n",
    "    else:\n",
    "        return \"-\"\n",
    "\n",
    "def num_help(log):\n",
    "    if log[\"condition\"] == \"treatment\":\n",
    "        if \"help\" in log[\"chatLog\"][\"current\"]:\n",
    "            return len(log[\"chatLog\"][\"current\"][\"help\"])\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return \"-\"\n",
    "\n",
    "def test_score(log):\n",
    "    score = 0\n",
    "    for quest in log[\"knowledgeAnswers\"]:\n",
    "        qlog = log[\"knowledgeAnswers\"][quest]\n",
    "        score += (qlog[\"answer\"] == qlog[\"solution\"])\n",
    "    return score\n",
    "\n",
    "def time_total(log):\n",
    "    learning = datetime.strptime(log[\"timestamps\"][\"enterMain\"], DATEFORMAT)\n",
    "    completion = datetime.strptime(log[\"timestamps\"][\"completion\"], DATEFORMAT)\n",
    "    return round((completion - learning).total_seconds() / 60.0, 1)\n",
    "\n",
    "def time_on_learning(log):\n",
    "    learning = datetime.strptime(log[\"timestamps\"][\"enterMain\"], DATEFORMAT)\n",
    "    knowledge = datetime.strptime(log[\"timestamps\"][\"enterKnowledge\"], DATEFORMAT)\n",
    "    return round((knowledge - learning).total_seconds() / 60.0, 1)\n",
    "\n",
    "def time_on_exam(log):\n",
    "    knowledge = datetime.strptime(log[\"timestamps\"][\"enterKnowledge\"], DATEFORMAT)\n",
    "    survey = datetime.strptime(log[\"timestamps\"][\"enterSurvey\"], DATEFORMAT)\n",
    "    return round((survey - knowledge).total_seconds() / 60.0, 1)\n",
    "\n",
    "def total_switches(log):\n",
    "    switches = 0\n",
    "    learning = datetime.strptime(log[\"timestamps\"][\"enterMain\"], DATEFORMAT)\n",
    "    if not \"tabSwitches\" in log:\n",
    "        return 0\n",
    "    for switch in log[\"tabSwitches\"]:\n",
    "        if switch[0] == \"exit\":\n",
    "            time = datetime.strptime(switch[1], DATEFORMAT)\n",
    "            if (learning < time):\n",
    "                switches += 1\n",
    "    return switches\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance df\n",
    "perf_df = pd.DataFrame()\n",
    "perf_df[\"surveyToken\"] = [l[\"surveyToken\"] for l in rlogs]\n",
    "perf_df[\"condition\"] = [l[\"condition\"] for l in rlogs]\n",
    "perf_df[\"completed_survey\"] = [completed_survey(l) for l in rlogs]\n",
    "perf_df[\"time_learning\"] = [time_on_learning(l) for l in rlogs]\n",
    "perf_df[\"num_messages\"] = [num_messages(l) for l in rlogs]\n",
    "perf_df[\"num_help\"] = [num_help(l) for l in rlogs]\n",
    "perf_df[\"test_score\"] = [test_score(l) for l in rlogs]\n",
    "perf_df[\"total_switches\"] = [total_switches(l) for l in rlogs]\n",
    "perf_df[\"exam_switches\"] = [exam_switches(l) for l in rlogs]\n",
    "perf_df[\"time_exam\"] = [time_on_exam(l) for l in rlogs]\n",
    "perf_df[\"time_total\"] = [time_total(l) for l in rlogs]\n",
    "perf_df[\"creation_time\"] = [l[\"timestamps\"][\"creation\"] for l in rlogs]\n",
    "perf_df[\"completion_time\"] = [l[\"timestamps\"][\"completion\"] for l in rlogs]\n",
    "\n",
    "\n",
    "perf_df = perf_df.sort_values(\"condition\")\n",
    "perf_df.to_csv(\"./tmp/perf_tmp.csv\", index=False)\n",
    "perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic survey responses (Q1-4 + Q18)\n",
    "base_df = pd.DataFrame()\n",
    "base_df[\"surveyToken\"] = [l[\"surveyToken\"] for l in rlogs]\n",
    "base_df[\"condition\"] = [l[\"condition\"] for l in rlogs]\n",
    "\n",
    "# basic answers\n",
    "for q in [\"q1\", \"q2\", \"q3\", \"q4\"]:\n",
    "    qtext = BeautifulSoup(rlogs[0][\"surveyAnswers\"][q][\"question\"], \"html\").text\n",
    "    base_df[qtext] = [l[\"surveyAnswers\"][q][\"answer\"] for l in rlogs]\n",
    "\n",
    "# get open response comments\n",
    "feedback = []\n",
    "qtext = BeautifulSoup(rlogs[0][\"surveyAnswers\"][\"q18\"][\"question\"], \"html\").text\n",
    "for log in rlogs:\n",
    "    if type(log[\"surveyAnswers\"][\"q18\"]) != str:\n",
    "        feedback.append(log[\"surveyAnswers\"][\"q18\"][\"answer\"])\n",
    "    else:\n",
    "        feedback.append(\"-\")\n",
    "base_df[qtext] = feedback\n",
    "\n",
    "base_df = base_df.sort_values(\"condition\")\n",
    "base_df.to_csv(\"./tmp/base_tmp.csv\", index=False)\n",
    "base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatment survey responses (Q5-17)\n",
    "treatment_df = pd.DataFrame()\n",
    "tlogs = [l for l in rlogs if l[\"condition\"] == \"treatment\"]\n",
    "treatment_df[\"surveyToken\"] = [l[\"surveyToken\"] for l in tlogs]\n",
    "treatment_df[\"condition\"] = [l[\"condition\"] for l in tlogs]\n",
    "\n",
    "for q in [\"q\" + str(i) for i in range(5, 18)]:\n",
    "    qtext = BeautifulSoup(tlogs[0][\"surveyAnswers\"][q][\"question\"], \"html\").text\n",
    "    treatment_df[qtext] = [l[\"surveyAnswers\"][q][\"answer\"] for l in tlogs]\n",
    "\n",
    "treatment_df = treatment_df.sort_values(\"condition\")\n",
    "treatment_df.to_csv(\"./tmp/treatment_tmp.csv\", index=False)\n",
    "treatment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographics survey responses\n",
    "demo_df = pd.DataFrame()\n",
    "demo_df[\"surveyToken\"] = [l[\"surveyToken\"] for l in rlogs]\n",
    "demo_df[\"condition\"] = [l[\"condition\"] for l in rlogs]\n",
    "\n",
    "for q in [\"q1\", \"q2\", \"q3\", \"q4\"]:\n",
    "    qtext = BeautifulSoup(tlogs[0][\"demographicsAnswers\"][q][\"question\"], \"html\").text\n",
    "    demo_df[qtext] = [l[\"demographicsAnswers\"][q][\"answer\"] for l in rlogs]\n",
    "\n",
    "demo_df = demo_df.sort_values(\"condition\")\n",
    "demo_df.to_csv(\"./tmp/demo_tmp.csv\", index=False)\n",
    "demo_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:  # set to True\n",
    "    for log in rlogs:\n",
    "        if log[\"condition\"] == \"treatment\":\n",
    "            print(log[\"surveyToken\"])\n",
    "            print(\"---------------------------------\")\n",
    "            compile_dialog(log)\n",
    "            print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-value between control group and tretement group for Q1 to Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic survey responses (Q1-4 + Q18)\n",
    "base_df = pd.DataFrame()\n",
    "base_df[\"surveyToken\"] = [l[\"surveyToken\"] for l in rlogs]\n",
    "base_df[\"condition\"] = [l[\"condition\"] for l in rlogs]\n",
    "\n",
    "# basic answers\n",
    "for q in [\"q1\", \"q2\", \"q3\", \"q4\"]:\n",
    "    qtext = BeautifulSoup(rlogs[0][\"surveyAnswers\"][q][\"question\"], \"html\").text\n",
    "    base_df[qtext] = [l[\"surveyAnswers\"][q][\"answer\"] for l in rlogs]\n",
    "\n",
    "base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \"Strongly Agree\" in base_df to number 7\n",
    "base_df = base_df.replace(\"Strongly Agree\", 5)\n",
    "base_df = base_df.replace(\"Strongly Disagree\", 1)\n",
    "base_df = base_df.replace(\"Disagree\", 2)\n",
    "base_df = base_df.replace(\"Agree\", 4)\n",
    "base_df = base_df.replace(\"Neutral\", 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the p-value for each question between the two conditions using mann-whitney u test\n",
    "from scipy.stats import mannwhitneyu\n",
    "p_q1_q4 = []\n",
    "for q in base_df.columns[2:]:\n",
    "    print(q)\n",
    "    p_q1_q4.append(mannwhitneyu(base_df[base_df[\"condition\"] == \"control\"][q], base_df[base_df[\"condition\"] == \"treatment\"][q]).pvalue)\n",
    "    print(mannwhitneyu(base_df[base_df[\"condition\"] == \"control\"][q], base_df[base_df[\"condition\"] == \"treatment\"][q]))\n",
    "p_q1_q4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a bar chart of the average score and std for each question in each condition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# get the average score for each question in each condition\n",
    "control_avg = []\n",
    "treatment_avg = []\n",
    "for q in base_df.columns[2:]:\n",
    "    control_avg.append(base_df[base_df[\"condition\"] == \"control\"][q].mean())\n",
    "    treatment_avg.append(base_df[base_df[\"condition\"] == \"treatment\"][q].mean())\n",
    "\n",
    "print(control_avg)\n",
    "print(treatment_avg)\n",
    "\n",
    "# get the std for each question in each condition\n",
    "control_std = []\n",
    "treatment_std = []\n",
    "for q in base_df.columns[2:]:\n",
    "    control_std.append(base_df[base_df[\"condition\"] == \"control\"][q].std())\n",
    "    treatment_std.append(base_df[base_df[\"condition\"] == \"treatment\"][q].std())\n",
    "\n",
    "# plot the bar chart\n",
    "x = np.arange(len(base_df.columns[2:]))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "rects1 = ax.bar(x - width / 2, control_avg, width, label=\"Control\", yerr=control_std)\n",
    "rects2 = ax.bar(x + width / 2, treatment_avg, width, label=\"Treatment\", yerr=treatment_std)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel(\"Average Score\")\n",
    "ax.set_title(\"Average Score by Question and Condition\")\n",
    "ax.set_xticks(x)\n",
    "\n",
    "# rotate the x-axis labels\n",
    "ax.set_xticklabels(base_df.columns[2:], rotation=30, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "# add the p-values to the plot\n",
    "# for i, v in enumerate(p_q1_q4):\n",
    "#     ax.text(i - width / 2, control_avg[i] + 0.1, str(round(v, 3)), color=\"blue\", fontweight=\"bold\")\n",
    "#     ax.text(i + width / 2, treatment_avg[i] + 0.1, str(round(v, 3)), color=\"blue\", fontweight=\"bold\")\n",
    "for i in range(len(p_q1_q4)):\n",
    "    if p_q1_q4[i] < 0.05:\n",
    "        ax.text(x[i] - width/2, max(control_avg[i], treatment_avg[i]) + 0.5, \"*\", fontsize=20)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_df = pd.DataFrame()\n",
    "tlogs = [l for l in rlogs if l[\"condition\"] == \"treatment\"]\n",
    "treatment_df[\"surveyToken\"] = [l[\"surveyToken\"] for l in tlogs]\n",
    "treatment_df[\"condition\"] = [l[\"condition\"] for l in tlogs]\n",
    "\n",
    "for q in [\"q\" + str(i) for i in range(5, 18)]:\n",
    "    qtext = BeautifulSoup(tlogs[0][\"surveyAnswers\"][q][\"question\"], \"html\").text\n",
    "    treatment_df[qtext] = [l[\"surveyAnswers\"][q][\"answer\"] for l in tlogs]\n",
    "    \n",
    "treatment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \"Strongly Agree\" in treatment_df to number 7\n",
    "treatment_df = treatment_df.replace(\"Strongly Agree\", 5)\n",
    "treatment_df = treatment_df.replace(\"Strongly Disagree\", 1)\n",
    "treatment_df = treatment_df.replace(\"Disagree\", 2)\n",
    "treatment_df = treatment_df.replace(\"Agree\", 4)\n",
    "treatment_df = treatment_df.replace(\"Neutral\", 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a bar chart of the average score and std for each question\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# get the average score for each question\n",
    "treatment_avg = []\n",
    "for q in treatment_df.columns[2:]:\n",
    "    treatment_avg.append(treatment_df[q].mean())\n",
    "\n",
    "print(treatment_avg)\n",
    "\n",
    "# get the std for each question\n",
    "treatment_std = []\n",
    "for q in treatment_df.columns[2:]:\n",
    "    treatment_std.append(treatment_df[q].std())\n",
    "\n",
    "# plot the bar chart\n",
    "x = np.arange(len(treatment_df.columns[2:]))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "rects1 = ax.bar(x, treatment_avg, width, label=\"Treatment\", yerr=treatment_std)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel(\"Average Score\")\n",
    "ax.set_title(\"Average Score by Question\")\n",
    "ax.set_xticks(x)\n",
    "\n",
    "# rotate the x-axis labels\n",
    "ax.set_xticklabels(treatment_df.columns[2:], rotation=30, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance df\n",
    "perf_df = pd.DataFrame()\n",
    "perf_df[\"surveyToken\"] = [l[\"surveyToken\"] for l in rlogs]\n",
    "perf_df[\"condition\"] = [l[\"condition\"] for l in rlogs]\n",
    "perf_df[\"completed_survey\"] = [completed_survey(l) for l in rlogs]\n",
    "perf_df[\"time_learning\"] = [time_on_learning(l) for l in rlogs]\n",
    "perf_df[\"num_messages\"] = [num_messages(l) for l in rlogs]\n",
    "perf_df[\"num_help\"] = [num_help(l) for l in rlogs]\n",
    "perf_df[\"test_score\"] = [test_score(l) for l in rlogs]\n",
    "perf_df[\"total_switches\"] = [total_switches(l) for l in rlogs]\n",
    "perf_df[\"exam_switches\"] = [exam_switches(l) for l in rlogs]\n",
    "perf_df[\"time_exam\"] = [time_on_exam(l) for l in rlogs]\n",
    "perf_df[\"time_total\"] = [time_total(l) for l in rlogs]\n",
    "perf_df[\"creation_time\"] = [l[\"timestamps\"][\"creation\"] for l in rlogs]\n",
    "perf_df[\"completion_time\"] = [l[\"timestamps\"][\"completion\"] for l in rlogs]\n",
    "\n",
    "perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace '-' with 0\n",
    "perf_df = perf_df.replace(\"-\", 0)\n",
    "perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the p-value for each column between the two conditions using t-test except columns 'creation_time' and 'completion_time'\n",
    "from scipy.stats import ttest_ind\n",
    "p_q5_q18 = []\n",
    "for q in perf_df.columns[3:-2]:\n",
    "    print(q)\n",
    "    print(perf_df[perf_df[\"condition\"] == \"control\"][q])\n",
    "    print(perf_df[perf_df[\"condition\"] == \"treatment\"][q])\n",
    "    p_q5_q18.append(ttest_ind(perf_df[perf_df[\"condition\"] == \"control\"][q], perf_df[perf_df[\"condition\"] == \"treatment\"][q])[1])\n",
    "    print(ttest_ind(perf_df[perf_df[\"condition\"] == \"control\"][q], perf_df[perf_df[\"condition\"] == \"treatment\"][q]))\n",
    "p_q5_q18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a bar chart of the average score and std for each question in each condition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# get the average score for each question in each condition from columns 'q5' to 'q18'\n",
    "control_avg = []\n",
    "for q in perf_df.columns[3:-2]:\n",
    "    control_avg.append(perf_df[perf_df[\"condition\"] == \"control\"][q].mean())\n",
    "\n",
    "treatment_avg = []\n",
    "for q in perf_df.columns[3:-2]:\n",
    "    treatment_avg.append(perf_df[perf_df[\"condition\"] == \"treatment\"][q].mean())\n",
    "\n",
    "print(control_avg)\n",
    "print(treatment_avg)\n",
    "\n",
    "# get the std for each question in each condition from columns 'q5' to 'q18'\n",
    "control_std = []\n",
    "for q in perf_df.columns[3:-2]:\n",
    "    control_std.append(perf_df[perf_df[\"condition\"] == \"control\"][q].std())\n",
    "\n",
    "treatment_std = []\n",
    "for q in perf_df.columns[3:-2]:\n",
    "    treatment_std.append(perf_df[perf_df[\"condition\"] == \"treatment\"][q].std())\n",
    "\n",
    "# plot the bar chart\n",
    "x = np.arange(len(perf_df.columns[3:-2]))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "rects1 = ax.bar(x - width/2, control_avg, width, label=\"Control\", yerr=control_std)\n",
    "rects2 = ax.bar(x + width/2, treatment_avg, width, label=\"Treatment\", yerr=treatment_std)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel(\"Average Score\")\n",
    "ax.set_title(\"Average Score by Question\")\n",
    "ax.set_xticks(x)\n",
    "\n",
    "# rotate the x-axis labels\n",
    "ax.set_xticklabels(perf_df.columns[3:-2], rotation=30, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "# add p-value to the plot\n",
    "for i in range(len(p_q5_q18)):\n",
    "    if p_q5_q18[i] < 0.05:\n",
    "        ax.text(x[i] - width/2, max(control_avg[i], treatment_avg[i]) + 0.5, \"*\", fontsize=20)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ck12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
